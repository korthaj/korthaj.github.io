<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Time complexity of recursive functions [Master theorem] · YourBasic </title>
  <meta name="description" content="You can often compute the time complexity of a recursive function by solving a recurrence relation. The master theorem gives solutions to a class of common recurrences.">
  
  <link rel="stylesheet" href="/style.css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Roboto+Mono:400,400i,700,700i&amp;subset=latin-ext" rel="stylesheet">
  <link rel="icon" type="image/png" href="/res/favicon-16x16.png">
  <link rel="icon" type="image/png" href="/res/favicon-32x32.png">
  <link rel="icon" type="image/png" href="/res/favicon-96x96.png">
  

  <script type="application/ld+json">
{
  "@context": "http://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yourbasic.org/algorithms/time-complexity-recursive-functions/"
  },
  "headline": "Time complexity of recursive functions [Master theorem]",
  "image": [
    "https://yourbasic.org/algorithms/recursive-soup.jpg"
   ],
  "datePublished": "2018-05-12T00:00:00&#43;0000",
  "dateModified": "2020-06-20T00:00:00&#43;0000",
  "author": {
    "@type": "Person",
    "name": "Stefan Nilsson"
  },
   "publisher": {
    "@type": "Organization",
    "name": "yourbasic.org",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yourbasic.org/res/favicon-96x96.png"
    }
  },
  "description": "You can often compute the time complexity of a recursive function by solving a recurrence relation. The master theorem gives solutions to a class of common recurrences."
}
</script>

<meta property="og:locale" content="en_US">
<meta property="og:type" content="article">
<meta property="og:title" content="Time complexity of recursive functions [Master theorem]">
<meta property="og:description" content="You can often compute the time complexity of a recursive function by solving a recurrence relation. The master theorem gives solutions to a class of common recurrences.">
<meta property="og:url" content="https://yourbasic.org/algorithms/time-complexity-recursive-functions/">
<meta property="og:image" content="https://yourbasic.org/algorithms/recursive-soup.jpg">



</head>

<body>
<header>
  <nav>
    <ul>
      <li><a href="/about/">About</a></li>
      <li><a href="/">Home</a></li>
      <li class="here"><a href="/algorithms/">Algorithms</a></li>
      <li><a href="/golang/">Go</a></li>
    </ul>
  </nav>
</header>

<main>
<article>
<h1>Time complexity of recursive functions [Master theorem]</h1>
<div class="tagline">yourbasic.org</div>
<p class="lead">It's often possible to compute the time complexity of a recursive function
by formulating and solving a recurrence relation.</p>
<!-- CC BY-SA 2.0: https://www.flickr.com/photos/gadl/337714905 -->
<div><img src="/algorithms/recursive-soup.jpg"></div>
<ul class="toc">
  <li><a href="#recurrence-relation">Recurrence relation (basic example)</a></li>
  <li><a href="#binary-search">Binary search</a></li>
  <li><a href="#master-theorem">Master theorem</a></li>
  <li><a href="#analysis-without-recurrence">Analysis without recurrence</a></li>
</ul>
<p>This text contains a few examples and a formula, the “master theorem”,
which gives the solution to a class of recurrence relations that
often show up when analyzing recursive functions.</p>
<p>We also show how to analyze recursive algorithms that depend on the size
and shape of a data structure.</p>
<h2 id="recurrence-relation">Recurrence relation</h2>
<p>As an introduction we show that the following recursive function
has linear time complexity.</p>
<pre><code>// Sum returns the sum 1 + 2 + ... + n, where n &gt;= 1.
func Sum(n int) int {
    if n == 1 {
        return 1
    }
    return n + Sum(n-1)
}
</code></pre>
<p>Let the function T(<i>n</i>) denote the number
of elementary operations performed by the function call <code>Sum(n)</code>.</p>
<p>We identify two properties of T(<i>n</i>).</p>
<ul>
<li>Since <code>Sum(1)</code> is computed using a fixed number of
operations <i>k</i><sub>1</sub>,
T(1) = <i>k</i><sub>1</sub>.</li>
<li>If <i>n</i> &gt; 1 the function will perform a fixed number
of operations <i>k</i><sub>2</sub>, and in addition,
it will make a recursive call to <code>Sum(n-1)</code>.
This recursive call will perform T(<i>n</i>-1) operations.
In total, we get
T(<i>n</i>) = <i>k</i><sub>2</sub> + T(<i>n</i>-1)</code>.</li>
</ul>
<p>If we are only looking for an asymptotic estimate of the time complexity,
we don&rsquo;t need to specify the actual values of
the constants <i>k</i><sub>1</sub>
and <i>k</i><sub>2</sub>.
Instead, we let <i>k</i><sub>1</sub> = <i>k</i><sub>2</sub> = 1.
To find the time complexity for the <code>Sum</code> function
can then be reduced to solving the recurrence relation</p>
<ul>
<li>T(1) = 1,  (*)</li>
<li>T(<i>n</i>) = 1 + T(<i>n</i>-1),
when <i>n</i> &gt; 1.  (**)</li>
</ul>
<p>By repeatedly applying these relations, we can compute T(<i>n</i>)
for any positive number <i>n</i>.</p>
<blockquote style="line-height:1.8em;">
T(<i>n</i>)&nbsp;=&nbsp;(**)&nbsp;<br>
1&nbsp;+&nbsp;T(<i>n</i>-1)&nbsp;=&nbsp;(**)&nbsp;<br>
1&nbsp;+&nbsp;(1&nbsp;+&nbsp;T(<i>n</i>-2))&nbsp;=
2&nbsp;+&nbsp;T(<i>n</i>-2)&nbsp;=&nbsp;(**)&nbsp;<br>
2&nbsp;+&nbsp;(1&nbsp;+&nbsp;T(<i>n</i>-3))&nbsp;=
3&nbsp;+&nbsp;T(<i>n</i>-3)&nbsp;=&nbsp;…&nbsp;<br>
k&nbsp;+&nbsp;T(<i>n</i>-<i>k</i>)&nbsp;=&nbsp;…&nbsp;<br>
n&nbsp;-&nbsp;1&nbsp;+&nbsp;T(1)&nbsp;=&nbsp;(*)&nbsp;<br>
n&nbsp;-&nbsp;1&nbsp;+&nbsp;1=&nbsp;&Theta;(<i>n</i>)
</blockquote>
<h2 id="binary-search">Binary search</h2>
<p>The very same method can be used also for more complex recursive algorithms.
Formulating the recurrences is straightforward,
but solving them is sometimes more difficult.</p>
<p>Let&rsquo;s try to compute the time complexity of this recursive implementation
of binary search.</p>
<pre><code>// Find returns the smallest index i at which x <= a[i].
// If there is no such index, it returns len(a).
// The slice must be sorted in ascending order.
func Find(a []int, x int) int {
	switch len(a) {
	case 0:
		return 0
	case 1:
		if x <= a[0] {
			return 0
		}
		return 1
	}
	mid := 1 + (len(a)-1)/2
	if x <= a[mid-1] {
		return Find(a[:mid], x)
	}
	return mid + Find(a[mid:], x)
}
</code></pre>
<p>We use the notation T(<i>n</i>) to mean the number of
elementary operations performed by this algorithm in the worst case,
when given a sorted slice of <i>n</i> elements.</p>
<p>Once again, we simplify the problem by only computing the asymptotic time complexity,
and let all constants be 1.
Then the recurrences become</p>
<ul>
<li>T(1) = 1,  (*)</li>
<li>T(<i>n</i>) = 1 + T(<i>n</i>/2),
when <i>n</i> &gt; 1.  (**)</li>
</ul>
<p>The equation (**) captures the fact that the function performs constant work
(that&rsquo;s the one) and a single recursive call to a slice of size <i>n</i>/2.</p>
<p>(In fact, the slice may also end up having <i>n</i>/2 + 1 elements.
We don&rsquo;t worry about that, since we&rsquo;re only looking for an asymptotic estimate.)</p>
<p>Once again, it&rsquo;s possible to find a solution by repeated substitution.</p>
<blockquote style="line-height:1.8em;">
T(<i>n</i>)&nbsp;=&nbsp;(**)<br>
1&nbsp;+&nbsp;T(<i>n</i>/2)&nbsp;=&nbsp;(**)<br>
1&nbsp;+&nbsp;(1&nbsp;+&nbsp;T(<i>n</i>/4))&nbsp;=&nbsp;2&nbsp;+&nbsp;T(<i>n</i>/4)&nbsp;=&nbsp;(**)<br>
2&nbsp;+&nbsp;(1&nbsp;+&nbsp;T(<i>n</i>/8))&nbsp;=&nbsp;3&nbsp;+&nbsp;T(<i>n</i>/8)&nbsp;=&nbsp;...<br>
k&nbsp;+&nbsp;T(<i>n</i>/2<sup>k</sup>)&nbsp;=&nbsp;...<br>
log&nbsp;n&nbsp;+&nbsp;T(<i>n</i>/2<sup>log&nbsp;<i>n</i></sup>)&nbsp;=&nbsp;log&nbsp;n&nbsp;+&nbsp;T(1)&nbsp;=&nbsp;(*)<br>
log&nbsp;n&nbsp;+&nbsp;1&nbsp;=&nbsp;&Theta;(log&nbsp;<i>n</i>).
</blockquote>
<h2 id="master-theorem">Master theorem</h2>
<p>The master theorem is a recipe that gives asymptotic estimates for a class of
recurrence relations that often show up when analyzing recursive algorithms.</p>
<p>Let a ≥ 1 and b &gt; 1
be constants, let f(<i>n</i>) be a function,
and let T(<i>n</i>) be a function over the positive numbers
defined by the recurrence</p>
<blockquote>
T(<i>n</i>)&nbsp;=&nbsp;aT(<i>n</i>/b)&nbsp;+&nbsp;f(<i>n</i>).
</blockquote>
<p>If f(<i>n</i>) = Θ(<i>n</i><sup>d</sup>),
where d ≥ 0, then</p>
<ul>
<li>T(<i>n</i>) = Θ(<i>n</i><sup>d</sup>)
if a &lt; b<sup>d</sup>,</li>
<li>T(<i>n</i>) = Θ(<i>n</i><sup>d</sup>log <i>n</i>)
if a = b<sup>d</sup>,</li>
<li>T(<i>n</i>) = Θ(<i>n</i><sup>log<sub>b</sub>a</sup>)
if a &gt; b<sup>d</sup>.</li>
</ul>
<p>We&rsquo;ll skip the proof. It isn&rsquo;t hard, but long.
In fact, you can use repeated substitution in the same way as in
the previous examples.</p>
<p>Let&rsquo;s check that the master theorem gives the correct solution
to the recurrence in the binary search example.
In this case a = 1,
 b = 2, and
the function f(<i>n</i>) = 1. This implies that
f(<i>n</i>) = Θ(<i>n</i><sup>0</sup>), i.e. d = 0.
We see that a = b<sup>d</sup>, and can use the second bullet point
of the master theorem to conclude that</p>
<blockquote>
T(<i>n</i>)&nbsp;=&nbsp;&Theta;(<i>n</i><sup>0</sup>log&nbsp;<i>n</i>),
</blockquote>
<p>which is correct.</p>
<h2 id="analysis-without-recurrence">Analysis without recurrence</h2>
<p>For algorithms that operate on a data structure, it&rsquo;s typically
not possible to find a recurrence relation.
Instead, we can count the work performed for each piece of the data structure
visited by the algorithm.</p>
<p>Consider this <a href="/algorithms/graph/">graph</a> with 36 (blue) vertices
and 3 disjoint connected components.</p>
<!-- Wikimedia Commons, http://en.wikipedia.org/wiki/File:Pseudoforest.svg -->
<img style="align:bottom;" src="/algorithms/forest.png">
<p style="clear: both;">
Depth-first search is an algorithm
that visits all edges in a graph&nbsp;<code>G</code> that belong to the
same connected component as vertex&nbsp;<emcode>v</code>.
</p>
<pre>
<b>Algorithm</b> DFS(G, v)
    <b>if</b> v is already visited
        <b>return</b>        
    Mark v as visited.
    // Perform some operation on v.
    <b>for</b> all neighbors x of v
        DFS(G, x)
</pre>
<p>The time complexity of this algorithm depends of the size and structure of the graph.
For example, if we start at the top left corner of our example graph,
the algorithm will visit only 4 edges.</p>
<p>To compute the time complexity, we can use the number of calls to <code>DFS</code>
as an elementary operation: the if statement and the mark operation both
run in constant time, and the for loop makes a single call to DFS for each iteration.</p>
<ul>
<li>Before running the algorithm, all |V| vertices must be marked as not visited.
This takes Θ(|V|) time.</li>
<li>Let E' be the set of all edges in the connected component visited by the algorithm.
The algorithm makes two calls to <code>DFS</code> for each edge {<i>u</i>, <i>v</i>} in E':
one time when the algorithm visits the neighbors of <i>u</i>,
and one time when it visits the neighbors of <i>v</i>.</li>
</ul>
<p>Hence, the time complexity of the algorithm is Θ(|V| + |E'|).</p>
<h3 id="further-examples">Further examples</h3>
<div><a href="/algorithms/graph/"><img src="/algorithms/social-graph-thumb.png" title="Introduction to graph algorithms: definitions and examples"></a></div>
<p>The <a href="/algorithms/graph/">Introduction to graph algorithms</a> article has more examples, including Dijkstra’s algorithm,
of this type of analysis.</p>


</article>
<aside>

    
  <h2>Related</h2>

  <div class="reference">
    <a href="/algorithms/time-complexity-explained/">How to analyze time complexity: Count your steps</a>
    <div class="desc">Time complexity analysis esti­mates the time to run an algo­rithm. It&#39;s calcu­lated by counting elemen­tary opera­tions.</div>
    <div class="source">yourbasic.org</div>
  </div>

  <div class="reference">
    <a href="/algorithms/big-o-notation-explained/">Big O notation: definition and examples</a>
    <div class="desc">Big O notation is a convenient way to describe how fast a function is growing. It is often used in computer science when estimating time complexity.</div>
    <div class="source">yourbasic.org</div>
  </div>

  <div class="reference">
    <a href="/algorithms/induction-recursive-functions/">On induction and recursive functions, with an application to binary search</a>
    <div class="desc">Mathematical induction can help you understand recursive functions better.</div>
    <div class="source">yourbasic.org</div>
  </div>

<h2>Most Read</h2>
    <div style="margin-top:1em;"><a href="/algorithms/time-complexity-explained/" title="How to analyse time complexity: Count your steps"><img src="/algorithms/abacus-mini.jpg"></a></div>
  <ul class="none">
  
    <li><a href="/algorithms/time-complexity-explained/">How to analyze time complexity: Count your steps</a></li>

    <li><a href="/algorithms/big-o-notation-explained/">Big O notation: definition and examples</a></li>

    <li><a href="/algorithms/dynamic-programming-explained/">Dynamic programming [step-by-step example]</a></li>

    <li><a href="/algorithms/loop-invariants-explained/">Loop invariants can give you coding superpowers</a></li>

    <li><a href="/algorithms/your-basic-api/">API design: principles and best practices</a></li>

    <li><a href="/algorithms/fastest-sorting-algorithm/">O(n log log n) time integer sorting</a></li>

  </ul>
  <p><a href="/algorithms/"><b>See all 24 algorithm articles</b></a></p>
</aside>
</main>

<footer>
  This work is licensed under&nbsp;a&nbsp;<a rel="license" alt="CC BY 3.0"
  href="http://creativecommons.org/licenses/by/3.0/"><span
  style="font-size:smaller;">CC&nbsp;BY&nbsp;3.0</span>&nbsp;license</a>.
</footer>
</body>
</html>
